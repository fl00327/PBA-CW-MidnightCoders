library(datasets)  # Load built-in datasets
library(datasets)  # Load built-in datasets
head(iris)         # Show the first six lines of iris data
head(iris)         # Show the first six lines of iris data
summary(iris)      # Summary statistics for iris data
plot(iris)         # Scatterplot matrix for iris data
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
head(iris)         # Show the first six lines of iris data
library(datasets)  # Load built-in datasets
head(iris)         # Show the first six lines of iris data
library(datasets)  # Load built-in datasets
head(iris)         # Show the first six lines of iris data
summary(iris)      # Summary statistics for iris data
plot(iris)         # Scatterplot matrix for iris data
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
# Then load the package by using either of the following:
require(pacman)  # Gives a confirmation message.
library(pacman)  # No message.
library(pacman)
# Or, by using "pacman::p_load" you can use the p_load
# function from pacman without actually loading pacman.
# These are packages I load every time.
pacman::p_load(pacman, dplyr, GGally, ggplot2, ggthemes,
ggvis, httr, lubridate, plotly, rio, rmarkdown, shiny,
stringr, tidyr)
# Or, by using "pacman::p_load" you can use the p_load
# function from pacman without actually loading pacman.
# These are packages I load every time.
pacman::p_load(pacman, dplyr, GGally, ggplot2, ggthemes,
ggvis, httr, lubridate, plotly, rio, rmarkdown, shiny,
stringr, tidyr)
library(datasets)  # Load/unload base packages manually
# Then load the package by using either of the following:
require(pacman)  # Gives a confirmation message.
library(pacman)  # No message.
# Or, by using "pacman::p_load" you can use the p_load
# function from pacman without actually loading pacman.
# These are packages I load every time.
pacman::p_load(pacman, dplyr, GGally, ggplot2, ggthemes,
ggvis, httr, lubridate, plotly, rio, rmarkdown, shiny,
stringr, tidyr)
library(datasets)  # Load/unload base packages manually
p_unload(all)  # Easier: clears all add-ons
detach("package:datasets", unload = TRUE)  # For base
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load/unload base packages manually
head(iris)
?plot  # Help for plot()
plot(iris$Species)  # Categorical variable
plot(iris$Petal.Length)  # Quantitative variable
plot(iris$Species, iris$Petal.Width)  # Cat x quant
plot(iris$Petal.Length, iris$Petal.Width)  # Quant pair
plot(iris)  # Entire data frame
# Plot with options
plot(iris$Petal.Length, iris$Petal.Width,
col = "#cc0000",  # Hex code for datalab.cc red
pch = 19,         # Use solid circles for points
main = "Iris: Petal Length vs. Petal Width",
xlab = "Petal Length",
ylab = "Petal Width")
plot(cos, 0, 2*pi)
plot(exp, 1, 5)
plot(dnorm, -3, +3)
# Formula plot with options
plot(dnorm, -3, +3,
col = "#cc0000",
lwd = 5,
main = "Standard Normal Distribution",
xlab = "z-scores",
ylab = "Density")
# Clear packages
detach("package:datasets", unload = TRUE)
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load built-in datasets
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)
?mtcars
Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (
# Clear console
cat("\014")  # ctrl+L
library(datasets)
?mtcars
head(mtcars)
barplot(mtcars$cyl)             # Doesn't work
faseeh <- 2*10
faseeh
# Need a table with frequencies for each category
cylinders <- table(mtcars$cyl)  # Create table
barplot(cylinders)              # Bar chart
plot(cylinders)                 # Default X-Y plot (lines)
# Clear environment
rm(list = ls())
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)
?iris
head(iris)
hist(iris$Sepal.Length)
hist(iris$Sepal.Width)
hist(iris$Petal.Length)
hist(iris$Petal.Width)
# Put graphs in 3 rows and 1 column
par(mfrow = c(3, 1))
# Histograms for each species using options
hist(iris$Petal.Width [iris$Species == "setosa"],
xlim = c(0, 3),
breaks = 9,
main = "Petal Width for Setosa",
xlab = "",
col = "red")
# Histograms for each species using options
hist(iris$Petal.Width [iris$Species == "setosa"],
xlim = c(0, 3),
breaks = 9,
main = "Petal Width for Setosa",
xlab = "",
col = "red")
hist(iris$Petal.Width [iris$Species == "versicolor"],
xlim = c(0, 3),
breaks = 9,
main = "Petal Width for Versicolor",
xlab = "",
col = "purple")
hist(iris$Petal.Width [iris$Species == "virginica"],
xlim = c(0, 3),
breaks = 9,
main = "Petal Width for Virginica",
xlab = "",
col = "blue")
# Restore graphic parameter
par(mfrow=c(1, 1))
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load/unload base packages manually
?mtcars
head(mtcars)
# Good to first check univariate distributions
hist(mtcars$wt)
hist(mtcars$mpg)
# Basic X-Y plot for two quantitative variables
plot(mtcars$wt, mtcars$mpg)
# Add some options
plot(mtcars$wt, mtcars$mpg,
pch = 19,         # Solid circle
cex = 1.5,        # Make 150% size
col = "#cc0000",  # Red
main = "MPG as a Function of Weight of Cars",
xlab = "Weight (in 1000 pounds)",
ylab = "MPG")
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load/unload base packages manually
ls(datasets)
# Annual Canadian Lynx trappings 1821-1934
?lynx
head(lynx)
# Default
hist(lynx)
# Add some options
hist(lynx,
breaks = 14,          # "Suggests" 14 bins
freq   = FALSE,       # Axis shows density, not freq.
col    = "thistle1",  # Color for histogram
main   = paste("Histogram of Annual Canadian Lynx",
"Trappings, 1821-1934"),
xlab   = "Number of Lynx Trapped")
# Add a normal distribution
curve(dnorm(x, mean = mean(lynx), sd = sd(lynx)),
col = "thistle4",  # Color of curve
lwd = 2,           # Line width of 2 pixels
add = TRUE)        # Superimpose on previous graph
# Add two kernel density estimators
lines(density(lynx), col = "blue", lwd = 2)
lines(density(lynx, adjust = 3), col = "purple", lwd = 2)
# Add a rug plot
rug(lynx, lwd = 2, col = "gray")
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load/unload base packages manually
head(iris)
summary(iris$Species)       # Categorical variable
summary(iris$Sepal.Length)  # Quantitative variable
summary(iris)               # Entire data frame
# Clear packages
detach("package:datasets", unload = TRUE)   # For base
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load base packages manually
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, dplyr, GGally, ggplot2, ggthemes,
ggvis, httr, lubridate, plotly, rio, rmarkdown, shiny,
stringr, tidyr)
head(iris)
# PSYCH PACKAGE ############################################
p_load(psych)
library(datasets)  # Load base packages manually
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, dplyr, GGally, ggplot2, ggthemes,
ggvis, httr, lubridate, plotly, rio, rmarkdown, shiny,
stringr, tidyr)
head(iris)
# PSYCH PACKAGE ############################################
p_load(psych)
# Get info on package
p_help(psych)           # Opens package PDF in browser
p_help(psych, web = F)  # Opens help in R Viewer
describe(iris$Sepal.Length)  # One quantitative variable
describe(iris)               # Entire data frame
# Clear environment
rm(list = ls())
# Clear packages
p_unload(all)  # Remove all add-ons
detach("package:datasets", unload = TRUE)   # For base
# Clear console
cat("\014")  # ctrl+L
1+2
2*%
2*5
# Clear console
cat("\014")  # ctrl+L
R.Version()
getwd()
setwd("~/Desktop/PBA-CW")
# Libraries
library(caret)
source("~/R/PBA-CW-MidnightCoders/model5_svm.R")
warnings()
debugSource("~/R/PBA-CW-MidnightCoders/model5_svm.R")
source("~/R/PBA-CW-MidnightCoders/model5_svm.R")
source("~/R/PBA-CW-MidnightCoders/model5_svm.R")
source("~/R/PBA-CW-MidnightCoders/model5_svm.R")
source("~/R/PBA-CW-MidnightCoders/model4_rf.R")
source("~/R/PBA-CW-MidnightCoders/model4_rf.R")
#For reproducability on submission, seed the random generator.
set.seed(12345)
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
fit1 <- train(weight_pounds ~ ., #Predict weight_pounds using all other fields in dataframe
data=dfTrain,      #Training dataset - 70% of original dataset
method='rf',       #Random Forest
trControl=control,  #Set repeated cross validation parameters
na.action=na.roughfix
)
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
#Normalising the data proved to reduce accuract for linear regression, therefore remove scalinging and centering.
preProcValues <- preProcess(df)
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
source("~/R/PBA-CW-MidnightCoders/model2_mars.R")
source("~/R/PBA-CW-MidnightCoders/model2_mars.R")
source("~/R/PBA-CW-MidnightCoders/model4_rf.R")
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
main()
debugSource("~/R/PBA-CW-MidnightCoders/model4_rf.R")
#Libraries
library(caret)
# ***************************************
#   Midnight Coders COM3018 Coursework
#         Support Vector Machine R File
#             November 2021
#
#        Random Forest to predict:
#             weight_pounds
#
#         Predictor Fields:
#             is_male
#            mother_age
#             pluarlity
#          gestation_weeks
#
# ***************************************
#Libraries
library(caret)
library(ggplot2)
#Remove all values in environment to save memory and provide a clear workspace.
rm(list = ls())
#For reproducability on submission, seed the random generator.
set.seed(12345)
#Load in file
df<-read.csv("./data/natality_1k.csv")
#Remove all the fields we do not want to use
df$X <-NULL
df$child_race <-NULL
df$mother_race<-NULL
df$cigarette_use <-NULL
df$cigarettes_per_day <-NULL
df$alcohol_use <-NULL
df$drinks_per_week <-NULL
df$weight_gain_pounds <-NULL
df$born_alive_alive <-NULL
df$born_alive_dead <-NULL
df$born_dead <-NULL
df$ever_born <-NULL
df$father_race <-NULL
df$father_age <-NULL
df$record_weight <-NULL
df$year <-NULL
df$wday <-NULL
df$state <-NULL
df$apgar_1min <-NULL
df$apgar_5min <-NULL
df$mother_residence_state <-NULL
df$lmp <-NULL
df$mother_married <-NULL
df$mother_birth_state <-NULL
df$day <-NULL
df$month <-NULL
df$source_year <-NULL
#Filter the data to ensure the values are correct
df <- dplyr::filter(df,
weight_pounds > 0,
mother_age > 0,
plurality > 0,
weight_pounds < 99,
mother_age < 99,
plurality < 99,
!is.na(is_male),
!is.na(plurality),
!is.na(mother_age),
!is.na(gestation_weeks),
!is.na(weight_pounds),
)
#Map the falues of "true" and "false" to numerical values.
df$is_male <- factor(df$is_male)
df$is_male <- as.numeric(df$is_male)
#Map weight_pounds to a true-false underweight field for classification
df$underweight <- ifelse(df$weight_pounds < 5.5, TRUE, FALSE)
#Remove the weight_pounds field as it is no longer needed.
df$weight_pounds <- NULL
df$underweight <- factor(df$underweight)
#Normalising the data proved to reduce accuract for linear regression, therefore remove scalinging and centering.
preProcValues <- preProcess(df)
#Create a split of the dataset (70:30 Train:Test), ensuring the distribution of weight_pounds remains the same for both subsets.
trainIndex <- createDataPartition(df$underweight, p=0.7, list=FALSE, times=1)
dfTrain <- df[trainIndex,]
dfTest <- df[-trainIndex,]
svmTuneGrid <- expand.grid(C = c(0,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5))
#Set parameters, such as cross validation. This samples 10 blocks of data 5 times, building a model. Then averages the results.
control <- trainControl(method='repeatedcv', number=100, repeats=1)
#Train a variety of models using the hyperparameter tuning to decide on the best value.
fit0 <- train(underweight ~ .,   #Predict weight_pounds using all other fields in dataframe
data=dfTrain,        #Training dataset - 70% of original dataset
method='svmLinear',      #Support Vector Machine ML Model
tuneGrid = svmTuneGrid, #Hyperparameter Tuning, changing c through 0-5 at various intervals
trControl=control)   #Set repeated cross validation parameters
#See what parameters were best
print(fit0)
fit1 <- train(underweight ~ .,   #Predict weight_pounds using all other fields in dataframe
data=dfTrain,        #Training dataset - 70% of original dataset
method='svmLinear',      #Support Vector Machine ML Model
trControl=control)   #Set repeated cross validation parameters
#Predict the outcome on a test set that the model has not yet seen.
predictions <- predict(fit1, dfTest)
# ***************************************
#   Midnight Coders COM3018 Coursework
#         Support Vector Machine R File
#             November 2021
#
#        Random Forest to predict:
#             weight_pounds
#
#         Predictor Fields:
#             is_male
#            mother_age
#             pluarlity
#          gestation_weeks
#
# ***************************************
main <- function(){
#Libraries
library(caret)
library(ggplot2)
#Remove all values in environment to save memory and provide a clear workspace.
rm(list = ls())
#For reproducability on submission, seed the random generator.
set.seed(12345)
#Load in file
df<-read.csv("./data/natality_1k.csv")
#Remove all the fields we do not want to use
df$X <-NULL
df$child_race <-NULL
df$mother_race<-NULL
df$cigarette_use <-NULL
df$cigarettes_per_day <-NULL
df$alcohol_use <-NULL
df$drinks_per_week <-NULL
df$weight_gain_pounds <-NULL
df$born_alive_alive <-NULL
df$born_alive_dead <-NULL
df$born_dead <-NULL
df$ever_born <-NULL
df$father_race <-NULL
df$father_age <-NULL
df$record_weight <-NULL
df$year <-NULL
df$wday <-NULL
df$state <-NULL
df$apgar_1min <-NULL
df$apgar_5min <-NULL
df$mother_residence_state <-NULL
df$lmp <-NULL
df$mother_married <-NULL
df$mother_birth_state <-NULL
df$day <-NULL
df$month <-NULL
df$source_year <-NULL
#Filter the data to ensure the values are correct
df <- dplyr::filter(df,
weight_pounds > 0,
mother_age > 0,
plurality > 0,
weight_pounds < 99,
mother_age < 99,
plurality < 99,
!is.na(is_male),
!is.na(plurality),
!is.na(mother_age),
!is.na(gestation_weeks),
!is.na(weight_pounds),
)
#Map the falues of "true" and "false" to numerical values.
df$is_male <- factor(df$is_male)
df$is_male <- as.numeric(df$is_male)
#Map weight_pounds to a true-false underweight field for classification
df$underweight <- ifelse(df$weight_pounds < 5.5, TRUE, FALSE)
#Remove the weight_pounds field as it is no longer needed.
df$weight_pounds <- NULL
df$underweight <- factor(df$underweight)
#Normalising the data proved to reduce accuract for linear regression, therefore remove scalinging and centering.
preProcValues <- preProcess(df)
#Create a split of the dataset (70:30 Train:Test), ensuring the distribution of weight_pounds remains the same for both subsets.
trainIndex <- createDataPartition(df$underweight, p=0.7, list=FALSE, times=1)
dfTrain <- df[trainIndex,]
dfTest <- df[-trainIndex,]
svmTuneGrid <- expand.grid(C = c(0,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5))
#Set parameters, such as cross validation. This samples 10 blocks of data 5 times, building a model. Then averages the results.
control <- trainControl(method='repeatedcv', number=100, repeats=1)
#Train a variety of models using the hyperparameter tuning to decide on the best value.
fit0 <- train(underweight ~ .,   #Predict weight_pounds using all other fields in dataframe
data=dfTrain,        #Training dataset - 70% of original dataset
method='svmLinear',      #Support Vector Machine ML Model
tuneGrid = svmTuneGrid, #Hyperparameter Tuning, changing c through 0-5 at various intervals
trControl=control)   #Set repeated cross validation parameters
#See what parameters were best
print(fit0)
fit1 <- train(underweight ~ .,   #Predict weight_pounds using all other fields in dataframe
data=dfTrain,        #Training dataset - 70% of original dataset
method='svmLinear',      #Support Vector Machine ML Model
trControl=control)   #Set repeated cross validation parameters
#Predict the outcome on a test set that the model has not yet seen.
predictions <- predict(fit1, dfTest)
}
main()
